{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Quantiphi Dog Breed Classification Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept of Transfer Learning\n",
    "Transfer learning refers to the technique of using knowledge of one domain to another domain.i.e. a NN model trained on one dataset can be used for other dataset by fine-tuning the former network.\n",
    "\n",
    "### When to use it?\n",
    "This is a function of several factors, but the two most important ones are the size of the new dataset (small or big), and its similarity to the original dataset (e.g. ImageNet-like in terms of the content of images and the classes, or very different). Our Case is:\n",
    "\n",
    "New dataset is small and somewhat similar to original dataset. Since the data is small, it is not a good idea to fine-tune the ConvNet due to overfitting concerns. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This approach is to leverage a network pre-trained on a large dataset. Such a network would have already learned features that are useful for most computer vision problems, and leveraging such features would allow us to reach a better accuracy than any method that would only rely on the available data.\n",
    "\n",
    "We will use the VGG16 architecture, pre-trained on the ImageNet dataset. Because the ImageNet dataset contains several \"Dog\" classes among its total of 1000 classes, this model will already have learned features that are relevant to our classification problem. \n",
    "\n",
    "In fact, it is possible that merely recording the softmax predictions of the model over our data rather than the bottleneck features would be enough to solve our dogs breed classification problem well. \n",
    "\n",
    "However, the method present here is more likely to generalize well to a broader range of problems, including problems featuring classes absent from ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications import xception\n",
    "from keras.applications import inception_v3\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "compare_loss={}\n",
    "compare_accuracy={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# check if all pre trained models are present\n",
    "start = dt.datetime.now()\n",
    "!ls ./input/keras-pretrained-models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'cp' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# copy all keras pre trained models to where keras/models exists\n",
    "!cp ./input/keras-pretrained-models/*notop* ~/.keras/models/\n",
    "!cp ./input/keras-pretrained-models/imagenet_class_index.json ~/.keras/models/\n",
    "!cp ./input/keras-pretrained-models/resnet50* ~/.keras/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# list the models copied\n",
    "!ls ~/.keras/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert image to array\n",
    "def read_img(address, size):\n",
    "    \"\"\"Read and resize image.\n",
    "    Returns Image as numpy array, by normalizing the values\n",
    "    \"\"\"\n",
    "    img = image.load_img(address, target_size=size)\n",
    "    img = image.img_to_array(img)\n",
    "    return img\n",
    "\n",
    "# function to convert labels to one hot encoding vector\n",
    "def OneHotEncoded(y_train):\n",
    "    y_t=np.zeros((len(y_train),Num_Class), dtype=int)\n",
    "    for i,x in enumerate(y_train):\n",
    "        y_t[i][int(x)-1]=1\n",
    "    return y_t\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#133 labels\n",
    "# Preprocess labels and create a csv file to keep track of labels, along with their encoded vector\n",
    "import os\n",
    "import csv\n",
    "from shutil import copy2\n",
    "INPUT_SIZE = 224\n",
    "directory_lists=os.listdir(\"./data/\")\n",
    "labels={}\n",
    "data={}\n",
    "with open('./input/dog-breed-identification/labels.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id','breed','encoded_value'])\n",
    "    for i in range(0,len(directory_lists)):\n",
    "        t=directory_lists[i].split(\".\")\n",
    "        imag=os.listdir(\"./data/\"+str(directory_lists[i]))\n",
    "        current_loc=\"./data/\"+str(directory_lists[i])+\"/\"\n",
    "        for k in range(0,len(imag)):\n",
    "            x=[current_loc+str(imag[k])]+[t[1]]+[t[0]]\n",
    "            writer.writerow(x)\n",
    "data_dir = './input/dog-breed-identification'\n",
    "labels = pd.read_csv(join(data_dir, 'labels.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is our First Method, Here we will Extract VGG16 bottleneck features, by forward passing the processed image data, and then using this forward pass data as input to logistic regression for classification.\n",
    "We don't need any form of preprocessing on our image data, that has been handled by preprocess_input function of VGG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8351it [01:18, 106.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Images shape: (8351, 224, 224, 3) size: 1,257,059,328\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction for our Network\n",
    "INPUT_SIZE = 224\n",
    "POOLING = 'avg'\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "x_train = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "y_train= np.zeros((len(labels),), dtype='float32')\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = preprocess_input(np.expand_dims(img.copy(), axis=0))\n",
    "    x_train[i] = x\n",
    "    y_train[i]=int(labels['encoded_value'][i])\n",
    "print('Train Images shape: {} size: {:,}'.format(x_train.shape, x_train.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Class=133   # number of classes in dataset\n",
    "# convert labels to one hot encoding\n",
    "y_train=OneHotEncoded(y_train)\n",
    "# split data in to training and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5595/5595 [==============================] - ETA: 11:0 - ETA: 6:1 - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 62s 11ms/step\n",
      "2756/2756 [==============================] - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 27s 10ms/step\n",
      "VGG train bottleneck features shape: (5595, 512) size: 2,864,640\n",
      "VGG valid bottleneck features shape: (2756, 512) size: 1,411,072\n"
     ]
    }
   ],
   "source": [
    "vgg_bottleneck = VGG16(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_vgg_bf = vgg_bottleneck.predict(X_train, batch_size=32, verbose=1)\n",
    "valid_vgg_bf = vgg_bottleneck.predict(X_val, batch_size=32, verbose=1)\n",
    "print('VGG train bottleneck features shape: {} size: {:,}'.format(train_vgg_bf.shape, train_vgg_bf.size))\n",
    "print('VGG valid bottleneck features shape: {} size: {:,}'.format(valid_vgg_bf.shape, valid_vgg_bf.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we just use last layer as logistic regression, basically one layered neural network(cross entropy). The accuracy we obtained is around 78.73%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation VGG LogLoss 1.2253131858872046\n",
      "Validation VGG Accuracy 0.7873730043541364\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=147)\n",
    "logreg.fit(train_vgg_bf, (y_train * range(Num_Class)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(valid_vgg_bf)\n",
    "valid_preds = logreg.predict(valid_vgg_bf)\n",
    "compare_loss['Vgg16']=log_loss(y_val, valid_probs)\n",
    "compare_accuracy['Vgg16']=accuracy_score((y_val * range(Num_Class)).sum(axis=1), valid_preds)\n",
    "print('Validation VGG LogLoss {}'.format(compare_loss['Vgg16']))\n",
    "print('Validation VGG Accuracy {}'.format(compare_accuracy['Vgg16']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we will test with Xception bottleneck features, and use these features to train logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8351it [01:42, 81.10it/s]\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 299\n",
    "POOLING = 'avg'\n",
    "x_train = np.zeros((len(labels), INPUT_SIZE, INPUT_SIZE, 3), dtype='float32')\n",
    "y_train= np.zeros((len(labels),), dtype='float32')\n",
    "\n",
    "for i, img_id in tqdm(enumerate(labels['id'])):\n",
    "    img = read_img(img_id, (INPUT_SIZE, INPUT_SIZE))\n",
    "    x = xception.preprocess_input(np.expand_dims(img.copy(), axis=0)) # preprocessing data for Xception model\n",
    "    x_train[i] = x\n",
    "    y_train[i]=int(labels['encoded_value'][i])\n",
    "    \n",
    "y_train=OneHotEncoded(y_train)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5595/5595 [==============================] - ETA: 5: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 113s 20ms/step\n",
      "2756/2756 [==============================] - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 50 - ETA: 49 - ETA: 50 - ETA: 52 - ETA: 51 - ETA: 50 - ETA: 49 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 50s 18ms/step\n",
      "Xception train bottleneck features shape: (5595, 2048) size: 11,458,560\n",
      "Xception valid bottleneck features shape: (2756, 2048) size: 5,644,288\n"
     ]
    }
   ],
   "source": [
    "# forward passing the training and validation set\n",
    "xception_bottleneck = xception.Xception(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_x_bf = xception_bottleneck.predict(X_train, batch_size=32, verbose=1)\n",
    "valid_x_bf = xception_bottleneck.predict(X_val, batch_size=32, verbose=1)\n",
    "print('Xception train bottleneck features shape: {} size: {:,}'.format(train_x_bf.shape, train_x_bf.size))\n",
    "print('Xception valid bottleneck features shape: {} size: {:,}'.format(valid_x_bf.shape, valid_x_bf.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Xception LogLoss 0.28439668076900365\n",
      "Validation Xception Accuracy 0.9132801161103048\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=147)\n",
    "logreg.fit(train_x_bf, (y_train * range(Num_Class)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(valid_x_bf)\n",
    "valid_preds = logreg.predict(valid_x_bf)\n",
    "compare_loss['Xception']=log_loss(y_val, valid_probs)\n",
    "compare_accuracy['Xception']=accuracy_score((y_val * range(Num_Class)).sum(axis=1), valid_preds)\n",
    "print('Validation Xception LogLoss {}'.format(compare_loss['Xception']))\n",
    "print('Validation Xception Accuracy {}'.format(compare_accuracy['Xception']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this step we have extracted Inception Bottleneck Features, and stacked them with Xception Features. This way we improved our input features.\n",
    "### Though Our Accuracy is looking almost same, but our loss reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5595/5595 [==============================] - ETA: 4: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 64s 12ms/step\n",
      "2756/2756 [==============================] - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 31s 11ms/step\n",
      "InceptionV3 train bottleneck features shape: (5595, 2048) size: 11,458,560\n",
      "InceptionV3 valid bottleneck features shape: (2756, 2048) size: 5,644,288\n"
     ]
    }
   ],
   "source": [
    "inception_bottleneck = inception_v3.InceptionV3(weights='imagenet', include_top=False, pooling=POOLING)\n",
    "train_i_bf = inception_bottleneck.predict(X_train, batch_size=32, verbose=1)\n",
    "valid_i_bf = inception_bottleneck.predict(X_val, batch_size=32, verbose=1)\n",
    "print('InceptionV3 train bottleneck features shape: {} size: {:,}'.format(train_i_bf.shape, train_i_bf.size))\n",
    "print('InceptionV3 valid bottleneck features shape: {} size: {:,}'.format(valid_i_bf.shape, valid_i_bf.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full train bottleneck features shape: (5595, 4096) size: 22,917,120\n",
      "Full valid bottleneck features shape: (2756, 4096) size: 11,288,576\n"
     ]
    }
   ],
   "source": [
    "X = np.hstack([train_x_bf, train_i_bf])\n",
    "V = np.hstack([valid_x_bf, valid_i_bf])\n",
    "print('Full train bottleneck features shape: {} size: {:,}'.format(X.shape, X.size))\n",
    "print('Full valid bottleneck features shape: {} size: {:,}'.format(V.shape, V.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Xception + Inception LogLoss 0.2805939375591576\n",
      "Validation Xception + Inception Accuracy 0.9172714078374455\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=1987)\n",
    "logreg.fit(X, (y_train * range(Num_Class)).sum(axis=1))\n",
    "valid_probs = logreg.predict_proba(V)\n",
    "valid_preds = logreg.predict(V)\n",
    "compare_loss['Xception_Inception']=log_loss(y_val, valid_probs)\n",
    "compare_accuracy['Xception_Inception']=accuracy_score((y_val * range(Num_Class)).sum(axis=1), valid_preds)\n",
    "print('Validation Xception + Inception LogLoss {}'.format(compare_loss['Xception_Inception']))\n",
    "print('Validation Xception + Inception Accuracy {}'.format(compare_accuracy['Xception_Inception']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can Conclude that the idea of using the stacked features from two different Networks was giving better results, we can also say that this is kind of method to ensemble. Using this, we were able to imporve both accuracy and reduce loss, though with less amount.\n",
    "\n",
    "### I have plotted graphs to show the loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(compare_loss.keys()), compare_loss.values(), color='b')\n",
    "plt.xticks(range(0,3),list(compare_loss.keys()))\n",
    "plt.title('Loss Graph')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(list(compare_accuracy.keys()), compare_accuracy.values(), color='g')\n",
    "plt.xticks(range(0,3),list(compare_accuracy.keys()))\n",
    "plt.title('Accuracy Graph')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
